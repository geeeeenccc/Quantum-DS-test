{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "N0y6dWNdRpJ4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the images (resize, convert to grayscale, and apply cloud mask)\n",
        "def process_image(img_path, cloud_mask_path=None, scale_factor=0.5):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, None, fx=scale_factor, fy=scale_factor)\n",
        "\n",
        "    # Apply cloud mask if provided(didn't really finish)\n",
        "    #if cloud_mask_path:\n",
        "    #    cloud_mask = cv2.imread(cloud_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    #    cloud_mask = cv2.resize(cloud_mask, (img.shape[1], img.shape[0]))  # Resize to match the image size\n",
        "    #    img = cv2.bitwise_and(img, img, mask=~cloud_mask)  # Apply the cloud mask\n",
        "\n",
        "    # Check if the image is already in grayscale\n",
        "    if len(img.shape) == 2:\n",
        "        gray_img = img\n",
        "    else:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    return img, gray_img"
      ],
      "metadata": {
        "id": "sunLOIJHQ9yQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code uses cv2_imshow to display images in Colab.\n",
        "\n",
        "Unfortunately, interactive features like panning won't work in Colab as it doesn't support mouse events."
      ],
      "metadata": {
        "id": "XQoGNg_oHW3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables for mouse callback\n",
        "panning = False\n",
        "start_x, start_y = -1, -1\n",
        "img_matches = None  # Define img_matches globally\n",
        "\n",
        "def mouse_callback(event, x, y, flags, param):\n",
        "    global panning, start_x, start_y, img_matches\n",
        "\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:\n",
        "        panning = True\n",
        "        start_x, start_y = x, y\n",
        "    elif event == cv2.EVENT_LBUTTONUP:\n",
        "        panning = False\n",
        "    elif event == cv2.EVENT_MOUSEMOVE:\n",
        "        if panning:\n",
        "            dx, dy = x - start_x, y - start_y\n",
        "            cv2.imshow(\"SIFT + BFMatcher\", cv2.warpAffine(img_matches, np.float32([[1, 0, dx], [0, 1, dy]]), (img_matches.shape[1], img_matches.shape[0])))\n"
      ],
      "metadata": {
        "id": "0Sm9PJngB8Ge"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to match images using SIFT and BFMatcher with cloud masking\n",
        "def match_images_sift_bf_with_mask(img1, img2, cloud_mask1=None, cloud_mask2=None, scale_factor=0.5, num_levels=3):\n",
        "    global img_matches  # Use the global variable\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "    bf = cv2.BFMatcher()\n",
        "\n",
        "    # Downscale images to a specific level\n",
        "    for _ in range(num_levels):\n",
        "        img1 = cv2.pyrDown(img1)\n",
        "        img2 = cv2.pyrDown(img2)\n",
        "\n",
        "        if cloud_mask1 is not None:\n",
        "            cloud_mask1 = cv2.resize(cloud_mask1, (img1.shape[1], img1.shape[0]))\n",
        "            cloud_mask1 = cv2.pyrDown(cloud_mask1)\n",
        "\n",
        "        if cloud_mask2 is not None:\n",
        "            cloud_mask2 = cv2.resize(cloud_mask2, (img2.shape[1], img2.shape[0]))\n",
        "            cloud_mask2 = cv2.pyrDown(cloud_mask2)\n",
        "\n",
        "    # Apply cloud mask to the grayscale images\n",
        "    if len(img1.shape) == 2:\n",
        "        img1_gray = img1\n",
        "    else:\n",
        "        img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    if cloud_mask1 is not None:\n",
        "        img1_gray = cv2.bitwise_and(img1_gray, img1_gray, mask=~cloud_mask1)\n",
        "\n",
        "    if len(img2.shape) == 2:\n",
        "        img2_gray = img2\n",
        "    else:\n",
        "        img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "    if cloud_mask2 is not None:\n",
        "        img2_gray = cv2.bitwise_and(img2_gray, img2_gray, mask=~cloud_mask2)\n",
        "\n",
        "    # Detect keypoints and compute descriptors\n",
        "    k1, d1 = sift.detectAndCompute(img1_gray, None)\n",
        "    k2, d2 = sift.detectAndCompute(img2_gray, None)\n",
        "\n",
        "    print(f'#keypoints in image1: {len(k1)}, image2: {len(k2)}')\n",
        "\n",
        "    # Match the keypoints\n",
        "    matches = bf.knnMatch(d1, d2, k=2)\n",
        "\n",
        "    # Apply ratio test\n",
        "    good_matches = []\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.7 * n.distance:\n",
        "            good_matches.append(m)\n",
        "\n",
        "    print(f'#good matches: {len(good_matches)} / {len(matches)}')\n",
        "\n",
        "    # Calculate the ratio of good matches to total matches\n",
        "    match_ratio = len(good_matches) / len(matches) if len(matches) > 0 else 0\n",
        "    print(f'Match Ratio: {match_ratio:.2%}')\n",
        "\n",
        "    # Visualize the matches\n",
        "    img_matches = cv2.drawMatches(\n",
        "        img1, k1, img2, k2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        "    )\n",
        "\n",
        "    # Highlight unrealistic matches in red among good matches\n",
        "    for m in good_matches:\n",
        "        pt1 = tuple(map(int, k1[m.queryIdx].pt))\n",
        "        pt2 = tuple(map(int, k2[m.trainIdx].pt))\n",
        "\n",
        "        # Check if the match is unrealistic\n",
        "        is_unrealistic = m not in matches\n",
        "\n",
        "        # Determine color based on match realism\n",
        "        color = (0, 0, 255) if is_unrealistic else (0, 255, 0)\n",
        "\n",
        "        cv2.line(img_matches, pt1, pt2, color, 1)  # Red color for unrealistic matches, green for good matches\n",
        "\n",
        "\n",
        "\n",
        "    # Create a named window and set the mouse callback\n",
        "    cv2.namedWindow(\"SIFT + BFMatcher\")\n",
        "    cv2.setMouseCallback(\"SIFT + BFMatcher\", mouse_callback)\n",
        "\n",
        "    # Display the visualization\n",
        "    cv2.imshow(\"SIFT + BFMatcher\", img_matches)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "0vjO_AinCA1z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link to the data:\n",
        "# https://zindi.africa/competitions/farm-pin-crop-detection-challenge/data\n",
        "\n",
        "# Paths to the input images (.jp2 format)\n",
        "img1_path = 'T34JEP_20170101T082332_TCI.jp2'\n",
        "img2_path = 'T34JEP_20170804T081559_TCI.jp2'\n",
        "\n",
        "# Load and process the images with cloud masking\n",
        "img1, gray_img1 = process_image(img1_path, )\n",
        "img2, gray_img2 = process_image(img2_path, )\n",
        "\n",
        "# Match the images using SIFT and BFMatcher with cloud masking at a specific level\n",
        "match_images_sift_bf_with_mask(gray_img1, gray_img2, cloud_mask1=None, cloud_mask2=None, num_levels=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlFV68d1CEww",
        "outputId": "c7f8c05a-dec4-47ed-bc3e-0690c8f846bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#keypoints in image1: 585, image2: 660\n",
            "#good matches: 65 / 585\n",
            "Match Ratio: 11.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we addressed the Farm Pin Crop Detection Challenge task, focusing on computer vision techniques for satellite image matching. Here are the key points and conclusions from our exploration:\n",
        "\n",
        "## Task Overview\n",
        "- The task involves matching satellite images, and we explored a computer vision approach using SIFT (Scale-Invariant Feature Transform) and FLANN (Fast Library for Approximate Nearest Neighbors) Matcher.\n",
        "- We considered the challenges associated with large-sized satellite images and the need to process them without losing quality.\n",
        "\n",
        "## Data Preparation\n",
        "- We loaded and processed the Sentinel-2 images from the provided dataset, considering the specific file structure and recommended files.\n",
        "- For the task of image matching, we organized a dataset for keypoints detection and image matching.\n",
        "\n",
        "## Keypoints Detection and Matching\n",
        "- Leveraging the SIFT algorithm, we detected keypoints and computed descriptors in the images.\n",
        "- Matches between keypoints were obtained using the FLANN Matcher, and we applied a ratio test to filter out reliable matches.\n",
        "- We visualized the matches, highlighting unrealistic matches in red for better inspection.\n",
        "\n",
        "## Future Considerations\n",
        "- The current approach provides a foundation for satellite image matching, but further refinement and experimentation could be beneficial.\n",
        "- Exploring deep learning models for this task, considering the unique challenges of satellite images, is a potential avenue for improvement.\n",
        "\n",
        "Overall, this notebook serves as a starting point for the Farm Pin Crop Detection Challenge, providing insights into the computer vision techniques employed for satellite image matching.\n"
      ],
      "metadata": {
        "id": "THQNV5EKHgSL"
      }
    }
  ]
}