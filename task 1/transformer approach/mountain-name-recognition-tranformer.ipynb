{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6998710,"sourceType":"datasetVersion","datasetId":4023260}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate\n!pip install seqeval","metadata":{"papermill":{"duration":28.152422,"end_time":"2023-11-12T07:58:00.337523","exception":false,"start_time":"2023-11-12T07:57:32.185101","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:14:13.296339Z","iopub.execute_input":"2023-11-23T22:14:13.296721Z","iopub.status.idle":"2023-11-23T22:14:41.218121Z","shell.execute_reply.started":"2023-11-23T22:14:13.296687Z","shell.execute_reply":"2023-11-23T22:14:41.217144Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=02345e6be580b823b95c9b0a61810dd18739127f04d3b7e1f5e2f96398e475e0\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport string\nfrom typing import List\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom itertools import chain\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport nltk\nimport spacy\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold, TimeSeriesSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split, Dataset\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import DataCollatorForTokenClassification\n\n%matplotlib inline","metadata":{"papermill":{"duration":18.620508,"end_time":"2023-11-12T07:58:18.969390","exception":false,"start_time":"2023-11-12T07:58:00.348882","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:14:41.219918Z","iopub.execute_input":"2023-11-23T22:14:41.220230Z","iopub.status.idle":"2023-11-23T22:15:02.105023Z","shell.execute_reply.started":"2023-11-23T22:14:41.220201Z","shell.execute_reply":"2023-11-23T22:15:02.104048Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='Token_classification_with_Transformers'></a>\n# Token classification with Transformers\n\nIn this notebook we'll attempt to use tranformers on our mountain dataset ","metadata":{"papermill":{"duration":0.010856,"end_time":"2023-11-12T07:58:18.992117","exception":false,"start_time":"2023-11-12T07:58:18.981261","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\n\ndf_mountains = pd.read_csv('/kaggle/input/mountain-ner-dataset/mountain_dataset_with_markup.csv', converters={'marker': eval})\n","metadata":{"papermill":{"duration":66.697407,"end_time":"2023-11-12T07:59:25.700571","exception":false,"start_time":"2023-11-12T07:58:19.003164","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.106634Z","iopub.execute_input":"2023-11-23T22:15:02.107748Z","iopub.status.idle":"2023-11-23T22:15:02.148015Z","shell.execute_reply.started":"2023-11-23T22:15:02.107708Z","shell.execute_reply":"2023-11-23T22:15:02.147242Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_mountains","metadata":{"execution":{"iopub.status.busy":"2023-11-23T22:15:02.150657Z","iopub.execute_input":"2023-11-23T22:15:02.151255Z","iopub.status.idle":"2023-11-23T22:15:02.173884Z","shell.execute_reply.started":"2023-11-23T22:15:02.151218Z","shell.execute_reply":"2023-11-23T22:15:02.173013Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   text      marker\n0     A visit to a science museum for hands-on learn...          []\n1     Voice surface coach set democratic time year. ...          []\n2     Parent according maybe activity activity finis...          []\n3     A visit to a sculpture garden with intriguing ...          []\n4     The Julian Alps in Slovenia offer pristine lak...  [(11, 15)]\n...                                                 ...         ...\n1579  They never audience meet. Appear region allow ...          []\n1580  Witnessing the mesmerizing Northern Lights dan...  [(75, 97)]\n1581  Consumer join stage. Best likely center they p...          []\n1582  Hospital real school cover hotel over. Any tra...          []\n1583  A brilliant pass from the midfielder sets up a...          []\n\n[1584 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>marker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A visit to a science museum for hands-on learn...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Voice surface coach set democratic time year. ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Parent according maybe activity activity finis...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A visit to a sculpture garden with intriguing ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Julian Alps in Slovenia offer pristine lak...</td>\n      <td>[(11, 15)]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1579</th>\n      <td>They never audience meet. Appear region allow ...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1580</th>\n      <td>Witnessing the mesmerizing Northern Lights dan...</td>\n      <td>[(75, 97)]</td>\n    </tr>\n    <tr>\n      <th>1581</th>\n      <td>Consumer join stage. Best likely center they p...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1582</th>\n      <td>Hospital real school cover hotel over. Any tra...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1583</th>\n      <td>A brilliant pass from the midfielder sets up a...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1584 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Our synthetic data is pretty clean so no need in such brutal cleaning\n# Yet this is a great example of how to process data from different sources like \n# Telegram Twitter Instagram etc.\ndef preprocess_text(text):\n    # Remove links\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n\n    # Special remove telegram links\n    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:t\\.me\\/\\S+|telegram\\.me\\/\\S+|telegram\\.dog\\/\\S+)\"\n    text = re.sub(pattern, '', text)\n\n    # Remove phone numbers\n    phone_regex = r'\\(?\\+?\\d{0,3}\\)?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{2}[-.\\s]?\\d{2}'\n    text = re.sub(phone_regex, '', text)\n\n    # Remove special characters\n    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n\n    # Remove tags\n    text = re.sub(r'@\\w+', '', text)\n\n    # Remove emojis\n    emoji_pattern = re.compile(\n        pattern=\"[\"\n                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                u\"\\U00002702-\\U000027B0\"\n                u\"\\U000024C2-\\U0001F251\"\n                u\"\\U0001f926-\\U0001f937\"\n                u'\\U00010000-\\U0010ffff'\n                u\"\\u200d\"\n                u\"\\u2640-\\u2642\"\n                u\"\\u2600-\\u2B55\"\n                u\"\\u23cf\"\n                u\"\\u23e9\"\n                u\"\\u231a\"\n                u\"\\u3030\"\n                \"]+\", flags=re.UNICODE\n    )\n    text = emoji_pattern.sub(r'', text)\n\n    # Remove multiple spaces\n    text = re.sub(r' +', ' ', text)\n\n    return text","metadata":{"papermill":{"duration":0.024233,"end_time":"2023-11-12T07:59:27.092231","exception":false,"start_time":"2023-11-12T07:59:27.067998","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.174921Z","iopub.execute_input":"2023-11-23T22:15:02.175196Z","iopub.status.idle":"2023-11-23T22:15:02.182350Z","shell.execute_reply.started":"2023-11-23T22:15:02.175172Z","shell.execute_reply":"2023-11-23T22:15:02.181411Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_mountains.shape","metadata":{"papermill":{"duration":0.020823,"end_time":"2023-11-12T07:59:27.125114","exception":false,"start_time":"2023-11-12T07:59:27.104291","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.183347Z","iopub.execute_input":"2023-11-23T22:15:02.183620Z","iopub.status.idle":"2023-11-23T22:15:02.199229Z","shell.execute_reply.started":"2023-11-23T22:15:02.183596Z","shell.execute_reply":"2023-11-23T22:15:02.198384Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(1584, 2)"},"metadata":{}}]},{"cell_type":"code","source":"def divide_markers(text, markers):\n    divided_markers = []\n    \n    for start, end in markers:\n        # Extract the mountain name from the text based on the marker\n        mountain_name = text[start:end]\n\n        # Split the mountain name into individual words\n        words = mountain_name.split(\" \")\n\n        # Generate divided markers for each word\n        for word in words:\n            word_start = text.find(word, start)\n            word_end = word_start + len(word)\n            divided_markers.append((word_start, word_end))\n\n    return divided_markers\n\n\n# Apply the divide_markers function to the DataFrame\ndf_mountains['loc_markers'] = df_mountains.apply(lambda row: divide_markers(row['text'], row['marker']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T22:15:02.200210Z","iopub.execute_input":"2023-11-23T22:15:02.200650Z","iopub.status.idle":"2023-11-23T22:15:02.237719Z","shell.execute_reply.started":"2023-11-23T22:15:02.200625Z","shell.execute_reply":"2023-11-23T22:15:02.236552Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_mountains","metadata":{"papermill":{"duration":0.029273,"end_time":"2023-11-12T07:59:27.191773","exception":false,"start_time":"2023-11-12T07:59:27.162500","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.238945Z","iopub.execute_input":"2023-11-23T22:15:02.239244Z","iopub.status.idle":"2023-11-23T22:15:02.258734Z","shell.execute_reply.started":"2023-11-23T22:15:02.239219Z","shell.execute_reply":"2023-11-23T22:15:02.257903Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                   text      marker  \\\n0     A visit to a science museum for hands-on learn...          []   \n1     Voice surface coach set democratic time year. ...          []   \n2     Parent according maybe activity activity finis...          []   \n3     A visit to a sculpture garden with intriguing ...          []   \n4     The Julian Alps in Slovenia offer pristine lak...  [(11, 15)]   \n...                                                 ...         ...   \n1579  They never audience meet. Appear region allow ...          []   \n1580  Witnessing the mesmerizing Northern Lights dan...  [(75, 97)]   \n1581  Consumer join stage. Best likely center they p...          []   \n1582  Hospital real school cover hotel over. Any tra...          []   \n1583  A brilliant pass from the midfielder sets up a...          []   \n\n               loc_markers  \n0                       []  \n1                       []  \n2                       []  \n3                       []  \n4               [(11, 15)]  \n...                    ...  \n1579                    []  \n1580  [(75, 87), (88, 97)]  \n1581                    []  \n1582                    []  \n1583                    []  \n\n[1584 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>marker</th>\n      <th>loc_markers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A visit to a science museum for hands-on learn...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Voice surface coach set democratic time year. ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Parent according maybe activity activity finis...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A visit to a sculpture garden with intriguing ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Julian Alps in Slovenia offer pristine lak...</td>\n      <td>[(11, 15)]</td>\n      <td>[(11, 15)]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1579</th>\n      <td>They never audience meet. Appear region allow ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1580</th>\n      <td>Witnessing the mesmerizing Northern Lights dan...</td>\n      <td>[(75, 97)]</td>\n      <td>[(75, 87), (88, 97)]</td>\n    </tr>\n    <tr>\n      <th>1581</th>\n      <td>Consumer join stage. Best likely center they p...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1582</th>\n      <td>Hospital real school cover hotel over. Any tra...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1583</th>\n      <td>A brilliant pass from the midfielder sets up a...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1584 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import spacy\n\nfrom spacy.training.iob_utils import biluo_to_iob, doc_to_biluo_tags\nfrom tqdm.autonotebook import tqdm\ntqdm.pandas()\n\ndf_mountains['loc_markers'] = df_mountains['loc_markers'].apply(lambda x: [[y[0], y[1], 'LOC'] for y in x])\n\nnlp = spacy.blank(\"xx\")\n\ndef convert_to_conll(row):\n    data = {\n        \"text\": row['text'],\n        \"label\": row['loc_markers']\n    }\n    doc = nlp(data[\"text\"])\n    ents = []\n\n    # Sort the spans based on their start positions\n    sorted_spans = sorted(data[\"label\"], key=lambda x: x[0])\n\n    for start, end, label in sorted_spans:\n        span = doc.char_span(start, end, label=label)\n\n        # Check for overlaps with existing spans\n        if span is not None:\n            if not any(span.start >= ent.start and span.end <= ent.end for ent in ents):\n                ents.append(span)\n        else:\n            pass\n            # TODO: fix not align to token case\n            # print(\"Skipping span (does not align to tokens):\", start, end, label, doc.text[start:end])\n\n    doc.ents = ents\n    return {\n        'tokens': list([t.text for t in doc]),\n        'labels': list(biluo_to_iob(doc_to_biluo_tags(doc)))\n    }\n\ndf_mountains['conll'] = df_mountains.progress_apply(convert_to_conll, axis=1)","metadata":{"papermill":{"duration":64.834354,"end_time":"2023-11-12T08:00:32.039569","exception":false,"start_time":"2023-11-12T07:59:27.205215","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.259945Z","iopub.execute_input":"2023-11-23T22:15:02.260222Z","iopub.status.idle":"2023-11-23T22:15:02.937220Z","shell.execute_reply.started":"2023-11-23T22:15:02.260197Z","shell.execute_reply":"2023-11-23T22:15:02.936270Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1584 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83c8836d77a49e181bcfdb858353423"}},"metadata":{}}]},{"cell_type":"code","source":"label2id = {'O': 0, 'B-LOC': 1, 'I-LOC': 2}\n\ndf_mountains['tokens'] = df_mountains.conll.str['tokens']\ndf_mountains['ner_tags'] = df_mountains.conll.str['labels'].apply(lambda x: [label2id[t] for t in x])\n\ndf_mountains['is_valid'] = 0\ndf_mountains.loc[df_mountains.index > 1200, 'is_valid'] = 1\n\ndf_train = df_mountains[df_mountains.is_valid == 0]\ndf_valid = df_mountains[df_mountains.is_valid == 1]","metadata":{"papermill":{"duration":1.450017,"end_time":"2023-11-12T08:00:33.503356","exception":false,"start_time":"2023-11-12T08:00:32.053339","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.942067Z","iopub.execute_input":"2023-11-23T22:15:02.942441Z","iopub.status.idle":"2023-11-23T22:15:02.962659Z","shell.execute_reply.started":"2023-11-23T22:15:02.942405Z","shell.execute_reply":"2023-11-23T22:15:02.961744Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_mountains[['tokens', 'ner_tags']].to_json(\n    'train_processed.json', orient='records', lines=True)\ndf_mountains[['tokens', 'ner_tags']].to_json(\n    'valid_processed.json', orient='records', lines=True)","metadata":{"papermill":{"duration":2.840495,"end_time":"2023-11-12T08:00:36.357760","exception":false,"start_time":"2023-11-12T08:00:33.517265","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:02.963832Z","iopub.execute_input":"2023-11-23T22:15:02.964217Z","iopub.status.idle":"2023-11-23T22:15:02.999964Z","shell.execute_reply.started":"2023-11-23T22:15:02.964190Z","shell.execute_reply":"2023-11-23T22:15:02.999228Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\n    \"json\",\n    data_files={\n        'train': 'train_processed.json',\n        'val': 'valid_processed.json'\n    }\n)\nraw_datasets","metadata":{"papermill":{"duration":1.513656,"end_time":"2023-11-12T08:00:37.885104","exception":false,"start_time":"2023-11-12T08:00:36.371448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:03.001016Z","iopub.execute_input":"2023-11-23T22:15:03.001284Z","iopub.status.idle":"2023-11-23T22:15:03.397636Z","shell.execute_reply.started":"2023-11-23T22:15:03.001259Z","shell.execute_reply":"2023-11-23T22:15:03.396773Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7e2e1697bb44d3b5/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401ac950c1ba4476ae4abfd76309b6ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f78bee0df24f689c3e324943a2c7c5"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7e2e1697bb44d3b5/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99e2d2d78fa41c6a6907371edec59b7"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 1584\n    })\n    val: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 1584\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\n\n\nid2label = {v: k for k, v in label2id.items()}\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    'xlm-roberta-base',\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large'\n#                                           , add_prefix_space=True\n                                         )","metadata":{"papermill":{"duration":10.379988,"end_time":"2023-11-12T08:00:48.279259","exception":false,"start_time":"2023-11-12T08:00:37.899271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:03.398811Z","iopub.execute_input":"2023-11-23T22:15:03.399088Z","iopub.status.idle":"2023-11-23T22:15:12.157539Z","shell.execute_reply.started":"2023-11-23T22:15:03.399063Z","shell.execute_reply":"2023-11-23T22:15:12.156535Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbf49e7e3af5466fb3313293b01b5ffd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b0ca5f28874d128055d32d62f284f1"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28276c6b6d4d49cb82ab87bbb93e18b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af058c5d576d4459b4bc362f2a6441c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b275427b0b4a5a929e1ff5def46d51"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n\ndef align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs","metadata":{"papermill":{"duration":0.030974,"end_time":"2023-11-12T08:00:48.358065","exception":false,"start_time":"2023-11-12T08:00:48.327091","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:12.159169Z","iopub.execute_input":"2023-11-23T22:15:12.159542Z","iopub.status.idle":"2023-11-23T22:15:12.170779Z","shell.execute_reply.started":"2023-11-23T22:15:12.159492Z","shell.execute_reply":"2023-11-23T22:15:12.169831Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets_ua = raw_datasets.map(\n    tokenize_and_align_labels,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)","metadata":{"papermill":{"duration":34.131206,"end_time":"2023-11-12T08:01:22.506278","exception":false,"start_time":"2023-11-12T08:00:48.375072","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:12.172008Z","iopub.execute_input":"2023-11-23T22:15:12.172557Z","iopub.status.idle":"2023-11-23T22:15:17.370141Z","shell.execute_reply.started":"2023-11-23T22:15:12.172522Z","shell.execute_reply":"2023-11-23T22:15:17.369195Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0c0e5098cc14b62bda10fa56da4213f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a04c699bad9492da1d60187f1513446"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"roberta-base\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=5\n)","metadata":{"papermill":{"duration":0.031459,"end_time":"2023-11-12T08:01:22.553331","exception":false,"start_time":"2023-11-12T08:01:22.521872","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:17.371412Z","iopub.execute_input":"2023-11-23T22:15:17.371791Z","iopub.status.idle":"2023-11-23T22:15:17.399068Z","shell.execute_reply.started":"2023-11-23T22:15:17.371756Z","shell.execute_reply":"2023-11-23T22:15:17.398191Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW([\n    {'params': list(model.roberta.parameters()), 'lr':  0.0000016},\n    {'params': list(model.classifier.parameters()), 'lr':  0.00012}\n])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0.1*3*(tokenized_datasets_ua['train'].num_rows/8),\n    num_training_steps=3*(tokenized_datasets_ua['train'].num_rows/8)\n)","metadata":{"papermill":{"duration":0.031575,"end_time":"2023-11-12T08:01:22.599783","exception":false,"start_time":"2023-11-12T08:01:22.568208","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:17.400217Z","iopub.execute_input":"2023-11-23T22:15:17.400511Z","iopub.status.idle":"2023-11-23T22:15:17.410309Z","shell.execute_reply.started":"2023-11-23T22:15:17.400474Z","shell.execute_reply":"2023-11-23T22:15:17.409378Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nmetric = evaluate.load(\"seqeval\")\n\nlabel_names = list(label2id.keys())\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n\n    # Remove ignored index (special tokens) and convert to labels\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": all_metrics[\"overall_precision\"],\n        \"recall\": all_metrics[\"overall_recall\"],\n        \"f1\": all_metrics[\"overall_f1\"],\n        \"accuracy\": all_metrics[\"overall_accuracy\"],\n    }","metadata":{"papermill":{"duration":2.824662,"end_time":"2023-11-12T08:01:25.439200","exception":false,"start_time":"2023-11-12T08:01:22.614538","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:17.411329Z","iopub.execute_input":"2023-11-23T22:15:17.411656Z","iopub.status.idle":"2023-11-23T22:15:19.969511Z","shell.execute_reply.started":"2023-11-23T22:15:17.411630Z","shell.execute_reply":"2023-11-23T22:15:19.968635Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725b60a3c14c48ec90daf7323c96e222"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"papermill":{"duration":12.244932,"end_time":"2023-11-12T08:01:37.699886","exception":false,"start_time":"2023-11-12T08:01:25.454954","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:19.971230Z","iopub.execute_input":"2023-11-23T22:15:19.972022Z","iopub.status.idle":"2023-11-23T22:15:31.578718Z","shell.execute_reply.started":"2023-11-23T22:15:19.971986Z","shell.execute_reply":"2023-11-23T22:15:31.577737Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.0)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"my_api_key = '9d4d0591e05d13690d35330a36ffa0de42a04006'","metadata":{"papermill":{"duration":0.024235,"end_time":"2023-11-12T08:01:37.739771","exception":false,"start_time":"2023-11-12T08:01:37.715536","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:31.580195Z","iopub.execute_input":"2023-11-23T22:15:31.580523Z","iopub.status.idle":"2023-11-23T22:15:31.585287Z","shell.execute_reply.started":"2023-11-23T22:15:31.580484Z","shell.execute_reply":"2023-11-23T22:15:31.584375Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\nimport wandb\n\nwandb.login(key=my_api_key)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets_ua[\"train\"],\n    eval_dataset=tokenized_datasets_ua[\"val\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    optimizers=(optimizer, scheduler)\n)\ntrainer.train()","metadata":{"papermill":{"duration":8448.055211,"end_time":"2023-11-12T10:22:25.811067","exception":false,"start_time":"2023-11-12T08:01:37.755856","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:15:31.586371Z","iopub.execute_input":"2023-11-23T22:15:31.586680Z","iopub.status.idle":"2023-11-23T22:18:34.604413Z","shell.execute_reply.started":"2023-11-23T22:15:31.586647Z","shell.execute_reply":"2023-11-23T22:18:34.603065Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgencgeray\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231123_221540-8angexjw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gencgeray/huggingface/runs/8angexjw' target=\"_blank\">treasured-surf-65</a></strong> to <a href='https://wandb.ai/gencgeray/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gencgeray/huggingface' target=\"_blank\">https://wandb.ai/gencgeray/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gencgeray/huggingface/runs/8angexjw' target=\"_blank\">https://wandb.ai/gencgeray/huggingface/runs/8angexjw</a>"},"metadata":{}},{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [990/990 02:21, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.053923</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.978682</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.029125</td>\n      <td>0.490085</td>\n      <td>0.429280</td>\n      <td>0.457672</td>\n      <td>0.988587</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.222000</td>\n      <td>0.025578</td>\n      <td>0.553435</td>\n      <td>0.719603</td>\n      <td>0.625674</td>\n      <td>0.993525</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.222000</td>\n      <td>0.025578</td>\n      <td>0.553435</td>\n      <td>0.719603</td>\n      <td>0.625674</td>\n      <td>0.993525</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.222000</td>\n      <td>0.025578</td>\n      <td>0.553435</td>\n      <td>0.719603</td>\n      <td>0.625674</td>\n      <td>0.993525</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=990, training_loss=0.1258959856900302, metrics={'train_runtime': 174.5843, 'train_samples_per_second': 45.365, 'train_steps_per_second': 5.671, 'total_flos': 141587377365744.0, 'train_loss': 0.1258959856900302, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"roberta-base\")","metadata":{"papermill":{"duration":2.819889,"end_time":"2023-11-12T10:22:28.681057","exception":false,"start_time":"2023-11-12T10:22:25.861168","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:18:34.606692Z","iopub.execute_input":"2023-11-23T22:18:34.607155Z","iopub.status.idle":"2023-11-23T22:18:37.500934Z","shell.execute_reply.started":"2023-11-23T22:18:34.607102Z","shell.execute_reply":"2023-11-23T22:18:37.499797Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"There will be no saved wheights as roberta (even base one) is pretty heavy","metadata":{}},{"cell_type":"code","source":"# For inference if needed\n# from transformers import pipeline\n\n# # Replace this with your own checkpoint\n# model_checkpoint = \"roberta-base\"\n# token_classifier = pipeline(\n#     \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n# )","metadata":{"papermill":{"duration":4.158039,"end_time":"2023-11-12T10:22:44.176516","exception":false,"start_time":"2023-11-12T10:22:40.018477","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-23T22:18:37.502459Z","iopub.execute_input":"2023-11-23T22:18:37.503417Z","iopub.status.idle":"2023-11-23T22:18:49.059925Z","shell.execute_reply.started":"2023-11-23T22:18:37.503379Z","shell.execute_reply":"2023-11-23T22:18:49.056266Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Notebook Summary:\n\n1. **Model and Tokenizer Loading:**\n   - Loaded a token classification model (`AutoModelForTokenClassification`) and tokenizer (`AutoTokenizer`) from the Hugging Face Transformers library.\n   - Used the `xlm-roberta-base` model with specific label mapping.\n\n2. **Data Processing:**\n   - Defined functions for aligning labels with tokens and tokenizing input data.\n   - Applied these functions to preprocess the dataset, aligning labels with tokenized inputs.\n\n3. **Training Setup:**\n   - Defined training arguments using `TrainingArguments`, specifying batch sizes, evaluation strategy, and number of epochs.\n   - Configured an optimizer (`AdamW`) and a learning rate scheduler.\n\n4. **Evaluation Metrics:**\n   - Utilized the `seqeval` library for computing precision, recall, F1 score, and accuracy during model evaluation.\n\n5. **WandB Integration:**\n   - Integrated WandB for experiment tracking during model training.\n\n6. **Trainer Setup and Training:**\n   - Configured the `Trainer` with the loaded model, training arguments, datasets, data collator, and evaluation metrics.\n   - Initiated model training using the `trainer.train()` method.\n\n7. **Conclusion and Saving Model:**\n   - Logged in to WandB using the provided API key.\n   - Executed training and saved the trained model.\n\n8. **Inference (Optional):**\n   - Provided a commented-out section for setting up a pipeline for token classification inference.\n\n9. **Summary and Future Steps:**\n   - The notebook focuses on fine-tuning a token classification model for the task of Named Entity Recognition (NER) on mountain names.\n   - It saves the trained model for future use.\n\n### Conclusion:\n\nThe notebook covers essential aspects of training a token classification model for NER. It demonstrates proficiency in using the Hugging Face Transformers library and includes experiment tracking with WandB. ","metadata":{}}]}
